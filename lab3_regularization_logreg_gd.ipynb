{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shashwatsaket46/study/blob/main/lab3_regularization_logreg_gd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lZ1lFnIFE3O"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: LASSO and Gradient Descent\n",
        "\n",
        "In this part of the lab, we will implement L1-regularized linear regression and solve it using gradient descent. First, let us generate the data. We will apply `StandardScaler()` to the input features to make sure they are normalized which will make optimization easier."
      ],
      "metadata": {
        "id": "eBFGNnMYIkjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_data = 500\n",
        "n_train = 400\n",
        "n_test = n_data - n_train\n",
        "n_features = 10\n",
        "\n",
        "X, y = make_regression(n_samples=n_data, n_features=n_features-1, noise=1, random_state=42)\n",
        "X = np.concat([X, np.ones((n_data, 1))], axis=1)\n",
        "X = StandardScaler().fit_transform(X)\n",
        "y -= y.mean()\n",
        "y = y / np.std(y)\n",
        "y = y.reshape(-1)\n",
        "\n",
        "\n",
        "X_tr, X_te = X[:n_train], X[n_train:]\n",
        "y_tr, y_te = y[:n_train], y[n_train:]\n",
        "\n",
        "f, arr = plt.subplots(2, 5, figsize=(10, 5))\n",
        "for i in range(10):\n",
        "  arr[i // 5, i % 5].plot(X_tr[:, i], y_tr, \"bo\")\n",
        "  arr[i // 5, i % 5].set_xlabel(f\"x_{i}\")\n",
        "  arr[i // 5, i % 5].set_ylabel(\"y\")\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "print(f\"Dataset shape: X_tr={X_tr.shape}, y_tr={y_tr.shape}, X_te={X_te.shape}, y_te={y_te.shape}\")"
      ],
      "metadata": {
        "id": "KImxjvP0IhYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that in $L_1$-regularized linear regression, the model predictions are given by\n",
        "$$\n",
        "\\hat y = x^T w,\n",
        "$$\n",
        "where $\\hat y \\in \\mathbb R$, $w \\in \\mathbb R^d$ and $x \\in \\mathbb R^d$. The loss is given by\n",
        "$$\n",
        "L(w) = \\frac 1 n \\|y - X w\\|^2 + \\lambda \\|w\\|_1,\n",
        "$$\n",
        "where $n$ is the number of datapoints, $y \\in \\mathbb R^n$, $X \\in \\mathbb R^{n \\times d}$. Now, let us implement the loss in numpy.\n",
        "\n",
        "Complete the function below. The loss you should get is approximately 6."
      ],
      "metadata": {
        "id": "KPMnVxkBLXC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(X, y, w, lam):\n",
        "    \"\"\"\n",
        "    Compute the total loss for L1-regularized linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : numpy.ndarray of shape (n, d)\n",
        "        Feature matrix where n is number samples and d is number of features\n",
        "    y : numpy.ndarray of shape (n,)\n",
        "        Target values\n",
        "    w : numpy.ndarray of shape (d,)\n",
        "        Weight vector\n",
        "    lam : float\n",
        "        L1 regularization strength lambda (scalar)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    loss: float\n",
        "        Combined MSE and L1 penalty, scalar\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: YOUR CODE HERE!\n",
        "\n",
        "    return loss\n",
        "\n",
        "lam = 0.1\n",
        "w = np.ones((n_features,))\n",
        "compute_loss(X_tr, y_tr, w, lam)"
      ],
      "metadata": {
        "id": "YLA_3ZFlJOuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to minimize this loss using gradient descent. The next step is to derive and implement the gradient of the loss: $\\nabla_w L(w)$.\n",
        "\n",
        "**Hint**: check the derivation of the gradient for the l2-regularized linear regression as we did in lecture 3 (slide 24 [here](https://docs.google.com/presentation/d/1_HRFPT9uRl8deCtj-DF1eNX_luFXmr2asUnHaBzf5MQ/edit?usp=sharing)). Be careful with the normalization: in the lecture we did not have the $\\frac 1 n$ scaling factor for the data fit term $\\|Xw - y\\|^2$!\n",
        "\n",
        "Use the identities\n",
        "- $\\frac{\\partial}{\\partial w}(w^TAw) = 2Aw$ for symmetric matrix $A$\n",
        "- $\\frac{\\partial}{\\partial w}(a^Tw) = a$ for vector $a$\n",
        "\n",
        "**Hint**: The l1-regularization term is non-smooth when some of the components of $w$ are 0, you can assume that all components are non-zero here. You may get intuition for what the gradient will look by studying the derivative $\\frac{d|u|}{du}$ for one-dimensional $u \\in \\mathbb R$."
      ],
      "metadata": {
        "id": "KssVsOvcOvaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(X, y, w, lam):\n",
        "    \"\"\"\n",
        "    Compute gradient of L1-regularized linear regression loss.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : numpy.ndarray of shape (n, d)\n",
        "        Feature matrix where n is number samples and d is number of features\n",
        "    y : numpy.ndarray of shape (n,)\n",
        "        Target values\n",
        "    w : numpy.ndarray of shape (d,)\n",
        "        Weight vector\n",
        "    lam : float\n",
        "        L1 regularization strength lambda (scalar)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    gradient : numpy.ndarray of shape (d,)\n",
        "        Gradient of the loss function with respect to w\n",
        "    \"\"\"\n",
        "    # TODO: YOUR CODE HERE!\n",
        "\n",
        "    # return gradient\n",
        "\n",
        "\n",
        "w = np.random.randn(n_features,)\n",
        "compute_gradient(X_tr, y_tr, w, lam)"
      ],
      "metadata": {
        "id": "de8_CoBiN5xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check the implementation of the gradient numerically by comparing it to a finite difference approximation. Run the code below. If you implemented both the loss and the gradient correctly, you should get a small number below $10^{-6}$."
      ],
      "metadata": {
        "id": "6juFq4UqTNV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import check_grad\n",
        "\n",
        "lam = 0.1\n",
        "w0 = np.random.randn(n_features,)\n",
        "\n",
        "def f(w):\n",
        "  return compute_loss(X_tr, y_tr, w, lam)\n",
        "\n",
        "def g(w):\n",
        "  return compute_gradient(X_tr, y_tr, w, lam)\n",
        "\n",
        "print(check_grad(f, g, w0))"
      ],
      "metadata": {
        "id": "WeQ2pG0lSJD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we computed the gradient, we can do gradient descent! Recall that the update rule is given by:\n",
        "$$w_{k+1} = w_k - \\alpha \\nabla_w L(w_k).$$\n",
        "\n",
        "Please implement gradient descent by completing the function below. Make use of the functions `compute_gradient` and `compute_loss` that you implemented above.\n",
        "\n",
        "Along with the final value of w, return the list of values of the loss $L(w_k)$ for each iteration $k$."
      ],
      "metadata": {
        "id": "VdW_GGF4T3Ys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w_init, lam, alpha, num_iter=10000\n",
        "                     ):\n",
        "    \"\"\"\n",
        "    Compute gradient of L1-regularized linear regression loss.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : numpy.ndarray of shape (n, d)\n",
        "        Feature matrix where n is number samples and d is number of features\n",
        "    y : numpy.ndarray of shape (n,)\n",
        "        Target values\n",
        "    w_init : numpy.ndarray of shape (d,)\n",
        "        Initial weight vector\n",
        "    lam : float\n",
        "        L1 regularization strength lambda\n",
        "    num_iter: int\n",
        "      number of iterations of gradient decent to run\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    w_final : numpy.ndarray of shape (d,)\n",
        "        Final value of parameters w\n",
        "    loss_history: list[float]\n",
        "        List of loss values at each iteration\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize weights and loss history\n",
        "    w = w_init.copy()  # Make a copy to avoid modifying the input\n",
        "    loss_history = []\n",
        "\n",
        "    # Perform gradient descent\n",
        "    for i in range(num_iter):\n",
        "      # TODO: YOUR CODE HERE!\n",
        "      ...\n",
        "\n",
        "    return w, loss_history\n",
        "\n",
        "\n",
        "w_init = np.ones((n_features,))\n",
        "w, loss_history = gradient_descent(X_tr, y_tr, w_init, lam=0.1, alpha=.001)\n",
        "\n",
        "f, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "xs = np.arange(len(loss_history))\n",
        "ys = np.array(loss_history)\n",
        "\n",
        "ax.plot(xs, ys)\n",
        "ax.grid()\n",
        "ax.set_xlabel(\"Step\")\n",
        "ax.set_ylabel(\"Loss\")\n"
      ],
      "metadata": {
        "id": "2tK-TUhcTHoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run gradient descent with different learning rates. Add your code for these experiments below.\n",
        "\n",
        "Explain the results: write a couple of sentences on what do you see depending on the different learning rate and why.\n",
        "\n",
        "**Add your explanaiton here:**"
      ],
      "metadata": {
        "id": "yF_WE59Ubwwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that l1 linear regression will set some of the parameters of w to zero. Let's see how many parameters will be set to zero as a function of regularization strength. Run the cell below."
      ],
      "metadata": {
        "id": "BdDWT6eHeKSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lambdas = [0., 1e-1, 3e-1, 1]\n",
        "num_zeros = []\n",
        "\n",
        "for lam in lambdas:\n",
        "  w_init = np.ones((n_features,))\n",
        "  w, loss_history = gradient_descent(X_tr, y_tr, w_init, lam=lam, alpha=.1, num_iter=1000)\n",
        "  num_zeros.append(np.sum(w < 1.e-6))\n",
        "\n",
        "\n",
        "\n",
        "f, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "xs = lambdas\n",
        "ys = num_zeros\n",
        "\n",
        "ax.plot(xs, ys, \"-bo\", mec=\"k\")\n",
        "ax.grid()\n",
        "ax.set_xlabel(\"regularization strength lambda\")\n",
        "ax.set_ylabel(\"number of zeros in w*\")"
      ],
      "metadata": {
        "id": "gX5FtHpLcSeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain the results: write a couple of sentences on what do you see depending on the different value of $\\lambda$ and why.\n",
        "\n",
        "**Add your explanaiton here:**"
      ],
      "metadata": {
        "id": "Pf7-VmRdtLbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Logistic Regression via Stochastic Gradient Descent\n",
        "\n",
        "This part of the lab demonstrates multi-class logistic regression (softmax regression) using PyTorch.\n",
        "We'll classify handwritten digits from the MNIST dataset using a linear model with\n",
        "L2 regularization and analyze the impact of regularization strength on model performance."
      ],
      "metadata": {
        "id": "KPX5tL8sedfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "mXqOZrD7dPku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "Qylyc1isgcQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's load the data. In pytorch, we will use [DataLoader](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html) objects which allow us to iterate over the dataset producing batches of training examples instead of working with the full dataset directly. This will make it convenient to implement SGD.\n",
        "\n",
        "Read the code below carefully and make sure you understand it."
      ],
      "metadata": {
        "id": "PLJnqw_Rghem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_mnist_data(batch_size=64):\n",
        "    \"\"\"\n",
        "    Load and preprocess MNIST dataset.\n",
        "\n",
        "    MNIST contains 28x28 grayscale images of handwritten digits (0-9).\n",
        "    We'll flatten the images to 784-dimensional vectors for logistic regression.\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): Number of samples per batch\n",
        "\n",
        "    Returns:\n",
        "        train_loader, test_loader: DataLoader objects for training and testing\n",
        "    \"\"\"\n",
        "\n",
        "    # Define transformations: convert to tensor and normalize\n",
        "    # Normalization helps with gradient descent convergence\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),  # Convert PIL image to tensor and scale to [0,1]\n",
        "        transforms.Normalize((0.1307,), (0.3081,))  # Normalize with MNIST mean and std\n",
        "    ])\n",
        "\n",
        "    # Load training dataset\n",
        "    train_dataset = datasets.MNIST(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    # Load test dataset\n",
        "    test_dataset = datasets.MNIST(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    # Create data loaders for batch processing\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True  # Shuffle training data for better SGD performance\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False  # No need to shuffle test data\n",
        "    )\n",
        "\n",
        "    print(f\"Training samples: {len(train_dataset)}\")\n",
        "    print(f\"Test samples: {len(test_dataset)}\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "train_loader, test_loader = load_mnist_data()"
      ],
      "metadata": {
        "id": "GIqQNMgWgd_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the first batch and look at the shapes of the data it returns."
      ],
      "metadata": {
        "id": "eHtIs_aSiE6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))\n",
        "x, y = batch\n",
        "print(f\"{x.shape=}, {y.shape=}\")\n",
        "print(f\"{len(train_loader)=}\")"
      ],
      "metadata": {
        "id": "ZGSb7hMHiKPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we will iterate over the `train_loader`, each iteration will return a batch containing 64 images of shape `1x28x28` and the corresponding labels. The number of batches in the iterator is `60000 / 64 = 938` (where we round the division up). Let's look at the data."
      ],
      "metadata": {
        "id": "jEn9l9e6iVvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_mnist_samples(train_loader, num_samples=16):\n",
        "    \"\"\"\n",
        "    Visualize random samples from the MNIST dataset.\n",
        "\n",
        "    This function helps students understand what the input data looks like\n",
        "    before it gets flattened and fed into the logistic regression model.\n",
        "\n",
        "    Args:\n",
        "        train_loader: DataLoader for training data\n",
        "        num_samples: Number of samples to display (should be a perfect square)\n",
        "    \"\"\"\n",
        "    # Get a batch of data\n",
        "    data_iter = iter(train_loader)\n",
        "    images, labels = next(data_iter)\n",
        "\n",
        "    # Create subplot grid\n",
        "    rows = int(np.sqrt(num_samples))\n",
        "    cols = rows\n",
        "\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.suptitle('MNIST Dataset Samples', fontsize=16, fontweight='bold')\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        plt.subplot(rows, cols, i + 1)\n",
        "\n",
        "        # Convert tensor to numpy and remove channel dimension\n",
        "        # Shape: (1, 28, 28) -> (28, 28)\n",
        "        img = images[i].squeeze().numpy()\n",
        "\n",
        "        # Display image\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.title(f'Label: {labels[i].item()}', fontsize=12)\n",
        "        plt.axis('off')  # Remove axis ticks and labels\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Image shape: {images[0].shape}\")\n",
        "    print(f\"After flattening: {images[0].view(-1).shape}\")\n",
        "    print(f\"Data type: {images[0].dtype}\")\n",
        "    print(f\"Value range: [{images[0].min().item():.3f}, {images[0].max().item():.3f}]\")\n",
        "\n",
        "visualize_mnist_samples(train_loader)"
      ],
      "metadata": {
        "id": "3SAF46pfhO0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let us implement the logistic regression model as a `torch.nn.Module`. Carefully study the implementation below and make sure you understand how it works and what each line is doing."
      ],
      "metadata": {
        "id": "l_s4JUaRjG3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-class Logistic Regression (Softmax Regression) model.\n",
        "\n",
        "    This is a linear classifier that maps input features directly to class logits.\n",
        "    The softmax function converts logits to probabilities for multi-class classification.\n",
        "\n",
        "    Architecture: Input (784) -> Linear -> Output (10 classes)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=784, num_classes=10):\n",
        "        \"\"\"\n",
        "        Initialize the logistic regression model.\n",
        "\n",
        "        Args:\n",
        "            input_dim (int): Number of input features (784 for flattened 28x28 images)\n",
        "            num_classes (int): Number of output classes (10 for digits 0-9)\n",
        "        \"\"\"\n",
        "        super(LogisticRegression, self).__init__()\n",
        "\n",
        "        # Single linear layer: y = Wx + b\n",
        "        # This maps from 784-dimensional input to 10-dimensional output (logits)\n",
        "        self.linear = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "        # Initialize weights with small random values\n",
        "        # Good initialization is important for gradient descent convergence\n",
        "        nn.init.normal_(self.linear.weight, mean=0.0, std=0.01)\n",
        "        nn.init.zeros_(self.linear.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, 1, 28, 28)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Raw logits of shape (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        # Flatten the input images: (batch_size, 1, 28, 28) -> (batch_size, 784)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Apply linear transformation to get logits\n",
        "        logits = self.linear(x)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "G9XNuvUUhTFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The magic of PyTorch is that it allows us to compute gradients automatically! Consider the example below.\n",
        "\n",
        "Here, we use PyTorch to automatically compute the gradients of the quadratic form $w^T A w$. We derived it in the lecture, it took a bit of work. Now, we can compute the gradients with a few lines of code!"
      ],
      "metadata": {
        "id": "jMJPXDXpjiiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Create a 3D vector w with gradient tracking enabled\n",
        "w = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "\n",
        "# Create a random 3x3 matrix and make it symmetric\n",
        "A_random = torch.randn(3, 3)\n",
        "A = (A_random + A_random.T) / 2  # Make symmetric: A = (A + A^T) / 2\n",
        "\n",
        "print(\"Vector w:\")\n",
        "print(w)\n",
        "print(\"\\nSymmetric matrix A:\")\n",
        "print(A)\n",
        "\n",
        "# Compute the quadratic form: loss = w^T A w\n",
        "loss = torch.matmul(torch.matmul(w.T, A), w)\n",
        "\n",
        "print(f\"\\nQuadratic form w^T A w = {loss.item():.4f}\")\n",
        "\n",
        "# Compute gradients using backpropagation\n",
        "loss.backward()\n",
        "\n",
        "# Access the computed gradient\n",
        "print(f\"\\nGradient ∇_w(w^T A w):\")\n",
        "print(w.grad)\n",
        "\n",
        "# Verify manually: gradient of w^T A w is (A + A^T) w = 2Aw (since A is symmetric)\n",
        "expected_grad = 2 * torch.matmul(A, w.detach())\n",
        "print(f\"\\nExpected gradient (2Aw):\")\n",
        "print(expected_grad)\n",
        "\n",
        "print(f\"\\nGradients match: {torch.allclose(w.grad, expected_grad)}\")"
      ],
      "metadata": {
        "id": "W5BftPVtjfPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let us implement L2-regularized logistic regression loss."
      ],
      "metadata": {
        "id": "c4WU12CqmgF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def l2_regularized_cross_entropy_loss(logits, targets, model, lambda_reg=0.01):\n",
        "    \"\"\"\n",
        "    Compute cross-entropy loss with L2 regularization.\n",
        "\n",
        "    This function combines the standard cross-entropy loss with an L2 penalty\n",
        "    on the model weights to prevent overfitting. The total loss is:\n",
        "\n",
        "    Loss = CrossEntropy(logits, targets) + λ * ||W||²\n",
        "\n",
        "    where λ (lambda_reg) controls the strength of regularization.\n",
        "\n",
        "    Args:\n",
        "        logits (torch.Tensor): Raw model outputs (before softmax), shape (batch_size, num_classes)\n",
        "        targets (torch.Tensor): True class labels, shape (batch_size,)\n",
        "        model (nn.Module): The model (needed to access weights for regularization)\n",
        "        lambda_reg (float): L2 regularization strength\n",
        "\n",
        "    Returns:\n",
        "        total_loss: Combined loss used for backpropagation\n",
        "    \"\"\"\n",
        "    # Compute standard cross-entropy loss\n",
        "    # F.cross_entropy applies softmax internally and computes negative log-likelihood\n",
        "    ce_loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    # Compute L2 regularization penalty\n",
        "    # Sum of squares of all model parameters (weights and biases)\n",
        "    l2_penalty = torch.sum(model.linear.weight**2)\n",
        "\n",
        "    # Combine cross-entropy loss with L2 penalty\n",
        "    total_loss = ce_loss + lambda_reg * l2_penalty\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "model = LogisticRegression()\n",
        "data, targets = next(iter(train_loader))\n",
        "\n",
        "# Forward pass\n",
        "logits = model(data)\n",
        "\n",
        "# Compute loss (including L2 regularization)\n",
        "loss = l2_regularized_cross_entropy_loss(logits, targets, model)"
      ],
      "metadata": {
        "id": "3N5aaR2ej8wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print out the gradient of the loss we computed above on the given batch of data with respect to weights and biases of the model."
      ],
      "metadata": {
        "id": "RdyTKFZvm0Hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the loss...\n",
        "# TODO: Your code goes here!"
      ],
      "metadata": {
        "id": "BOndQHlMmY1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's train the model! We will use the SGD implementation from pytorch. Read the following code carefully and make sure you understand it: it will be very relevant when we start training neural networks!"
      ],
      "metadata": {
        "id": "jHL8r87Vol6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, lambda_reg=0.01, learning_rate=0.01, num_epochs=10):\n",
        "    \"\"\"\n",
        "    Train the logistic regression model using SGD with L2 regularization.\n",
        "\n",
        "    Args:\n",
        "        model: The neural network model\n",
        "        train_loader: DataLoader for training data\n",
        "        device: Device to run computations on\n",
        "        lambda_reg: L2 regularization strength\n",
        "        learning_rate: Learning rate for SGD\n",
        "        num_epochs: Number of training epochs\n",
        "\n",
        "    Returns:\n",
        "        lists of training losses per epoch\n",
        "    \"\"\"\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    # Create optimizer (SGD with momentum for better convergence)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(data)\n",
        "\n",
        "            # Compute loss using our custom function\n",
        "            total_loss = l2_regularized_cross_entropy_loss(\n",
        "                logits, targets, model, lambda_reg\n",
        "            )\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()  # Clear gradients from previous step\n",
        "            total_loss.backward()   # Compute gradients\n",
        "            optimizer.step()       # Update parameters\n",
        "\n",
        "            # Accumulate statistics\n",
        "            epoch_loss += total_loss.item()\n",
        "            train_losses.append(total_loss.item())\n",
        "\n",
        "            # Print progress every 200 batches\n",
        "            if batch_idx % 200 == 0:\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, '\n",
        "                      f'Loss: {total_loss.item():.4f}')\n",
        "\n",
        "        # Calculate epoch statistics\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}: Avg Loss = {avg_loss:.4f}')\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(\"Training completed!\")\n",
        "\n",
        "    return train_losses\n",
        "\n",
        "\n",
        "model = LogisticRegression()\n",
        "train_losses = train_model(model, train_loader, lambda_reg=0.01, learning_rate=0.001, num_epochs=2)\n",
        "\n",
        "plt.plot(train_losses)\n",
        "plt.xlabel(\"batches\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "rR-9QfyNm-D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let us evaluate the model that we trained! You will need to fist compute the predictions on each datapoint in the `test_loader` and store them in a numpy array. Make use of `model.forward` to compute the logits, and then convert them to predictions. You will also need to return the true class labels for all of the test examples."
      ],
      "metadata": {
        "id": "IotSGHN-o7xD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_predictions_labels(model, test_loader):\n",
        "    \"\"\"\n",
        "    Returns model predictions and true labels on the given dataloader.\n",
        "\n",
        "    Args:\n",
        "        model: Trained neural network model\n",
        "        test_loader: DataLoader for test data\n",
        "\n",
        "    Returns:\n",
        "        all_predictions: numpy array of shape (n_test_data,), all test predictions\n",
        "        all_targets: numpy array of shape (n_test_data,), all test ground truth labels\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode (disables dropout, etc.)\n",
        "\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    # Disable gradient computation for efficiency\n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "            # TODO: Your code goes here!\n",
        "            ...\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_targets = np.array(all_targets)\n",
        "    return all_predictions, all_targets\n",
        "\n",
        "all_predictions, all_targets = evaluate_predictions_labels(model, test_loader)\n",
        "print(f\"{all_predictions.shape=}, {all_targets.shape=}\")\n",
        "print(f\"{all_predictions.shape=}, {all_targets.shape=}\")"
      ],
      "metadata": {
        "id": "wpW9ChjZoKM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's implement the accuracy metric for this model."
      ],
      "metadata": {
        "id": "WCUUIVD5qT36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(all_predictions, all_targets):\n",
        "  # TODO: Your code goes here!\n",
        "  ...\n",
        "\n",
        "accuracy(all_predictions, all_targets)"
      ],
      "metadata": {
        "id": "vbfqQ31-qKWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now got our first MNIST classifier! You should see an accuracy around 85-90%.\n",
        "\n",
        "Now, experiment with the L2 regularization parameter in the model. What happens to accuracy if you set the regularization strength to 10? What if you set it to 0?\n",
        "\n",
        "Provide your code and write a couple of sentences explaining the results."
      ],
      "metadata": {
        "id": "9Ii2UeT8q2Wa"
      }
    }
  ]
}